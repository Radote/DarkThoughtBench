{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c78f5c9",
   "metadata": {},
   "source": [
    "# Reproduce our project\n",
    "\n",
    "This notebook contains all the steps to reproduce the results presented in our project. It covers dependency installation, API key setup, model evaluations, and results analysis/plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a244d",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "First, we install the project package itself along with its dependencies defined in `setup.py`. The `-e` flag installs it in \"editable\" mode, meaning changes to the source code are reflected immediately without needing reinstallation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31647c67",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import the necessary Python libraries for running the evaluations and displaying results within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from inspect_ai import eval\n",
    "from darkthoughtbench.task import darkthoughtbench\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fe773",
   "metadata": {},
   "source": [
    "## 3. Set Initial API Keys\n",
    "\n",
    "Set the environment variables for the API keys required for the initial evaluations and the subsequent analysis.\n",
    "\n",
    "* **`GOOGLE_API_KEY`**: Your Google AI Studio key (for Gemini). Needed now if `darkthoughtbench` uses Gemini as an overseer/scorer, and also later for the consistency analysis.\n",
    "* **`ANTHROPIC_API_KEY`**: Your Anthropic key (for Claude). Needed now if `darkthoughtbench` uses Claude as an overseer/scorer, and also later for the consistency analysis.\n",
    "* **`OPENAI_API_KEY`**: **Crucially, set this to your *DeepSeek* API key for now.** This is because the `inspect_ai.eval` calls below target the DeepSeek API endpoint (`model_base_url='https://api.deepseek.com'`), which uses the OpenAI client interface. We will update this variable later before running the consistency analysis script which calls the actual OpenAI API.\n",
    "\n",
    "**Important:** Replace `'your-...'` placeholders with your actual API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f61109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Replace placeholders with your actual keys!\n",
    "# Use your DeepSeek key for OPENAI_API_KEY here initially.\n",
    "os.environ['GOOGLE_API_KEY'] = 'your-google-api-key'\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'your-anthropic-api-key'\n",
    "os.environ['OPENAI_API_KEY'] = 'your-deepseek-api-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765582a",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation: DeepSeek-R1 (with CoT)\n",
    "\n",
    "Run the `inspect_ai` evaluation using the `darkthoughtbench` task on the DeepSeek-R1 model (`deepseek-reasoner`). We specify `model_is_reasoning=True` and use the DeepSeek API endpoint. The results are saved to a log file in the `../logs` directory, which is then renamed for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r1 = eval(tasks=darkthoughtbench(model_is_reasoning=True), model_base_url='https://api.deepseek.com', model='openai/deepseek-reasoner', log_dir='../logs')\n",
    "\n",
    "old_log_r1_path = log_r1[0].location\n",
    "log_r1_directory = os.path.dirname(old_log_r1_path)\n",
    "new_log_r1_filename = 'DeepSeek-R1_with_CoT.eval'\n",
    "new_log_r1_path = os.path.join(log_r1_directory, new_log_r1_filename)\n",
    "if not os.path.exists(new_log_r1_path):\n",
    "    os.rename(old_log_r1_path, new_log_r1_path)\n",
    "else:\n",
    "    print(f\"File '{new_log_r1_filename}' already exists. Skipping rename.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e47de9",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation: DeepSeek-V3\n",
    "\n",
    "Run the `inspect_ai` evaluation using the `darkthoughtbench` task on the DeepSeek-V3 model (`deepseek-chat`). We specify `model_is_reasoning=False`. The results are saved to a log file in the `../logs` directory and renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_v3 = eval(tasks=darkthoughtbench(model_is_reasoning=False), model_base_url='https://api.deepseek.com', model='openai/deepseek-chat', log_dir='../logs')\n",
    "\n",
    "old_log_v3_path = log_v3[0].location\n",
    "log_v3_directory = os.path.dirname(old_log_v3_path)\n",
    "new_log_v3_filename = 'DeepSeek-V3.eval'\n",
    "new_log_v3_path = os.path.join(log_v3_directory, new_log_v3_filename)\n",
    "if not os.path.exists(new_log_v3_path):\n",
    "    os.rename(old_log_v3_path, new_log_v3_path)\n",
    "else:\n",
    "    print(f\"File '{new_log_v3_filename}' already exists. Skipping rename.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02674eda",
   "metadata": {},
   "source": [
    "## 6. View Evaluation Logs (Optional)\n",
    "\n",
    "Use the `inspect view` command to launch a web-based UI where you can explore the evaluation logs generated in the previous steps. This requires navigating back to the parent directory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! (cd .. && inspect view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42947bf4",
   "metadata": {},
   "source": [
    "## 7. Update OpenAI API Key for Overseer Analysis\n",
    "\n",
    "Now, we update the `OPENAI_API_KEY` environment variable to use your **actual OpenAI API key**. This is necessary because the `CoT_response_consistency.py` script will directly call the OpenAI API (specifically GPT-4o) as one of the overseer models. The Google and Anthropic keys are already set correctly from Cell 3.\n",
    "\n",
    "**Important:** Replace `'your-openai-api-key'` with your actual OpenAI key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537dbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Replace placeholder with your actual OpenAI key!\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90314a9",
   "metadata": {},
   "source": [
    "## 8. Run CoT-Response Consistency Analysis\n",
    "\n",
    "Execute the Python script `CoT_response_consistency.py`. This script reads the `DeepSeek-R1_with_CoT.eval` log file generated earlier. It then uses the overseer models (GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet) via their respective APIs to analyze each conversation for consistency between the Chain-of-Thought (CoT) and the final response regarding dark patterns. The script generates JSON logs for each overseer's analysis and confusion matrix plots, saving them to the `../logs` and `../plots` directories, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadcd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../src/CoT_response_consistency.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf343f",
   "metadata": {},
   "source": [
    "## 9. Display Generated Plots\n",
    "\n",
    "Load and display the confusion matrix plots (`.png` files) generated by the `CoT_response_consistency.py` script from the `../plots` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../plots'\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith('.png'):\n",
    "        print(os.path.splitext(file)[0])\n",
    "        display(Image(filename=os.path.join(folder, file)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "darkbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
